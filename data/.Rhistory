plotFun(((exp(L)+1)^(-y-1))*exp(y*L) ~ y, y.lim=range(0,20), L=0,
ylab = "p(y|L)",
xlab = "y")
plotFun(((exp(L)+1)^(-y-1))*exp(y*L) ~ y, y.lim=range(0,20), L=0,
ylab = "p(y|L)",
xlab = "y",
title("L = 0"))
plotFun(((exp(L)+1)^(-y-1))*exp(y*L) ~ y, y.lim=range(0,20), L=1,
ylab = "p(y|L)",
xlab = "y")
plotFun(((exp(L)+1)^(-y-1))*exp(y*L) ~ y, y.lim=range(0,20), L=2,
ylab = "p(y|L)",
xlab = "y")
View(calcCI)
heart_train
source("/Users/eugenefung/Desktop/Monash Things/School Work/Y2S2/FIT2086/Assignment 3/my.prediction.stats.R")
source("/Users/eugenefung/Desktop/Monash Things/School Work/Y2S2/FIT2086/Assignment 3/wrappers.R")
install.packages("pROC")
install.packages("kknn")
library(glmnet)
library(rpart)
library(randomForest)
library(kknn)
library(pROC)
library(boot)
#Qn 2.1
heart_train =  read.csv("/Users/eugenefung/Desktop/Monash Things/School Work/Y2S2/FIT2086/Assignment 3/heart.train.ass3.2020.csv")
heart_train
summary(heart_train)
tree.heart_train = rpart(HD ~., heart_train)
tree.heart_train
plot(tree.heart_train)
text(tree.heart_train, digits=3)
#plot(tree.heart_train)
#text(tree.heart_train, digits=3)
cv = learn.tree.cv(HD~.,data=heart_train,nfolds=10,m=5000)
plot.tree.cv(cv)
cv
#Qn 2.1
heart_train =  read.csv("/Users/eugenefung/Desktop/Monash Things/School Work/Y2S2/FIT2086/Assignment 3/heart.train.ass3.2020.csv")
heart_train
summary(heart_train)
#tree.heart_train = rpart(HD ~., heart_train)
#tree.heart_train
#plot(tree.heart_train)
#text(tree.heart_train, digits=3)
cv = learn.tree.cv(HD~.,data=heart_train,nfolds=10,m=5000)
plot.tree.cv(cv)
cv
#Qn 2.2
plot(cv$best.tree)
text(cv$best.tree,pretty=12)
text(cv$best.tree, cex = 0.7, use.n = TRUE, fancy = FALSE, all = TRUE)
text(cv$best.tree,pretty=12,  cex = 0.7, use.n = TRUE, fancy = FALSE, all = TRUE)
#Qn 2.2
plot(cv$best.tree)
text(cv$best.tree,pretty=12,  cex = 0.7, use.n = TRUE, fancy = FALSE, all = TRUE)
text(cv$best.tree,pretty=12)
text(cv$best.tree,pretty=12)
#Qn 2.2
plot(cv$best.tree)
text(cv$best.tree,pretty=12)
#Qn 2.2
plot(cv$best.tree)
text(cv$best.tree)
text(cv$best.tree,pretty=12)
#Qn 2.2
plot(cv$best.tree)
text(cv$best.tree,pretty=15)
#Qn 2.2
plot(cv$best.tree)
text(cv$best.tree,pretty=15)
install.packages("pROC")
install.packages("pROC")
#Qn 2.5
fullmod = glm(HD ~., data = heart_train, family = binomial)
summary(fullmod)
fullmod_bic = step(fullmod, trace = 0, k = log(length(hd)), direction = "both")
fullmod_bic = step(fullmod, trace = 0, k = log(length(heart_train)), direction = "both")
summary(fullmod_bic)
install.packages(c("glmnet", "randomForest"))
#Qn 2.5
fullmod = glm(HD ~., data = heart_train, family = binomial)
summary(fullmod)
View(heart_train)
source("/Users/eugenefung/Desktop/Monash Things/School Work/Y2S2/FIT2086/Assignment 3/my.prediction.stats.R")
source("/Users/eugenefung/Desktop/Monash Things/School Work/Y2S2/FIT2086/Assignment 3/wrappers.R")
install.packages("pROC")
install.packages("pROC")
install.packages("kknn")
install.packages("kknn")
source("/Users/eugenefung/Desktop/Monash Things/School Work/Y2S2/FIT2086/Assignment 3/my.prediction.stats.R")
source("/Users/eugenefung/Desktop/Monash Things/School Work/Y2S2/FIT2086/Assignment 3/wrappers.R")
install.packages("pROC")
install.packages("kknn")
library(glmnet)
library(rpart)
library(randomForest)
library(kknn)
library(pROC)
library(boot)
#Qn 2.1
heart_train =  read.csv("/Users/eugenefung/Desktop/Monash Things/School Work/Y2S2/FIT2086/Assignment 3/heart.train.ass3.2020.csv")
heart_train
summary(heart_train)
#tree.heart_train = rpart(HD ~., heart_train)
#tree.heart_train
#plot(tree.heart_train)
#text(tree.heart_train, digits=3)
cv = learn.tree.cv(HD~.,data=heart_train,nfolds=10,m=5000)
source("/Users/eugenefung/Desktop/Monash Things/School Work/Y2S2/FIT2086/Assignment 3/my.prediction.stats.R")
source("/Users/eugenefung/Desktop/Monash Things/School Work/Y2S2/FIT2086/Assignment 3/wrappers.R")
install.packages("pROC")
install.packages("pROC")
install.packages("kknn")
library(glmnet)
library(rpart)
library(randomForest)
library(kknn)
library(pROC)
library(boot)
#Qn 2.1
heart_train =  read.csv("/Users/eugenefung/Desktop/Monash Things/School Work/Y2S2/FIT2086/Assignment 3/heart.train.ass3.2020.csv")
heart_train
summary(heart_train)
#tree.heart_train = rpart(HD ~., heart_train)
#tree.heart_train
#plot(tree.heart_train)
#text(tree.heart_train, digits=3)
cv = learn.tree.cv(HD~.,data=heart_train,nfolds=10,m=5000)
plot.tree.cv(cv)
cv
#Qn 2.2
plot(cv$best.tree)
text(cv$best.tree,pretty=12)
#Qn 2.5
fullmod = glm(HD ~., data = heart_train, family = binomial)
summary(fullmod)
fullmod_bic = step(fullmod, trace = 0, k = log(length(heart_train)), direction = "both")
summary(fullmod_bic)
#Qn 2.5
fullmod = glm(HD ~., data=heart_train, family=binomial)
summary(fullmod)
step_fullmod_bic = step(fullmod, k = log(length(heart_train)), direction = "both", trace = 0)
summary(step_fullmod_bic)
cv
summary(cv)
plot.tree.cv(cv)
#Qn 2.5
fullmod = glm(HD ~., data=heart_train, family=binomial)
summary(fullmod)
step_fullmod_bic = step(fullmod, k = log(length(heart_train)), direction = "both", trace = 0)
summary(step_fullmod_bic)
#Qn 2.7
heart_test = read.csv("/Users/eugenefung/Desktop/Monash Things/School Work/Y2S2/FIT2086/Assignment 3/heart.test.ass3.2020.csv")
heart_test
prob = predict(step_fullmod_bic, heart_test, type="response")
my.pred.stats(prob, heart_test$HD)
heart_test$HD
my.pred.stats(predict(cv$best.tree, heart_test)[,2], heart_test$HD)
my.pred.stats(prob, heart_test$HD)
heart_test$HD
my.pred.stats(prob, heart_test$HD)
my.pred.stats(predict(cv$best.tree, heart_test)[,2], heart_test$HD)
my.pred.stats(prob, heart_test$HD)
my.pred.stats(prob, heart_test$HD)
my.pred.stats(predict(cv$best.tree, heart_test)[,2], heart_test$HD)
#Qn 2.8
heart_test
table(predict(cv$best.tree, heart_test, type = "class"), heart_test$HD)
pred = factor(prob>1/2,c(F,T),c("N","Y"))
table(pred, heart_test$HD)
cv.$best.tree
cv$best.tree
#Qn 2.8
heart_test
table(predict(cv$best.tree, heart_test, type = "class"), heart_test$HD)
pred = factor(prob>1/2,c(F,T),c("N","Y"))
table(pred, heart_test$HD)
View(heart_test)
#Qn 2.9
my.pred.stats <- function(prob, target, display = TRUE)
{
rv = list()
classes = levels(target)
# Convert probabilities to best guesses at classes
pred = factor(prob > 1/2, c(F,T), classes)
# Compute statistics
T = table(pred,target)
roc.obj = roc(response=as.numeric(target)-1, as.vector(prob), quiet=TRUE)
rv$ca   = mean(pred==target)
rv$sens = T[2,2]/(T[1,2]+T[2,2])
rv$spec = T[1,1]/(T[1,1]+T[2,1])
rv$auc  = as.numeric(roc.obj$auc)
# Prob is probability of success, so if the target is not a success, flip the probability
# to get probability of failure
prob[target==classes[1]] = 1 - prob[target==classes[1]]
# Also make sure we never get exactly zero or one for probabilities due to numerical rounding
prob = (prob+1e-10)/(1+2e-10)
rv$log.loss = -sum(log(prob))
# Display, if requested
if (display == TRUE)
{
cat("---------------------------------------------------------------------------\n")
cat("Performance statistics:\n")
cat("\n")
cat("Confusion matrix:\n\n")
print(T)
cat("\n")
cat("Classification accuracy =", rv$ca, "\n")
cat("Sensitivity             =", rv$sens, "\n")
cat("Specificity             =", rv$spec, "\n")
cat("Area-under-curve        =", rv$auc, "\n")
cat("Logarithmic loss        =", rv$log.loss, "\n")
cat("\n")
#plot(roc.obj)
cat("---------------------------------------------------------------------------\n")
}
else
{
#
return(rv)
}
}
boot.auc = function(formula, data, indices)
{
# Create a bootstrapped version of our data
d = data[indices,]
# Fit a logistic regression to the bootstrapped data
fit = glm(formula, d, family=binomial)
# Compute the AUC and return it
target = as.character(fit$terms[[2]])
rv = my.pred.stats(predict(fit,d,type="response"), d[,target], display = F)
return(rv$auc)
}
bs = boot(data=heart_test, statistic=boot.auc, R=5000, formula=HD ~.)
boot.ci(bs,conf=0.95,type="bca")
plot(bs)
#Qn 2.9
my.pred.stats <- function(prob, target, display = TRUE)
{
rv = list()
classes = levels(target)
# Convert probabilities to best guesses at classes
pred = factor(prob > 1/2, c(F,T), classes)
# Compute statistics
T = table(pred,target)
roc.obj = roc(response=as.numeric(target)-1, as.vector(prob), quiet=TRUE)
rv$ca   = mean(pred==target)
rv$sens = T[2,2]/(T[1,2]+T[2,2])
rv$spec = T[1,1]/(T[1,1]+T[2,1])
rv$auc  = as.numeric(roc.obj$auc)
# Prob is probability of success, so if the target is not a success, flip the probability
# to get probability of failure
prob[target==classes[1]] = 1 - prob[target==classes[1]]
# Also make sure we never get exactly zero or one for probabilities due to numerical rounding
prob = (prob+1e-10)/(1+2e-10)
rv$log.loss = -sum(log(prob))
# Display, if requested
if (display == TRUE)
{
cat("---------------------------------------------------------------------------\n")
cat("Performance statistics:\n")
cat("\n")
cat("Confusion matrix:\n\n")
print(T)
cat("\n")
cat("Classification accuracy =", rv$ca, "\n")
cat("Sensitivity             =", rv$sens, "\n")
cat("Specificity             =", rv$spec, "\n")
cat("Area-under-curve        =", rv$auc, "\n")
cat("Logarithmic loss        =", rv$log.loss, "\n")
cat("\n")
#plot(roc.obj)
cat("---------------------------------------------------------------------------\n")
}
else
{
#
return(rv)
}
}
boot.auc = function(formula, data, indices)
{
# Create a bootstrapped version of our data
d = data[indices,]
# Fit a logistic regression to the bootstrapped data
fit = glm(formula, d, family=binomial)
# Compute the AUC and return it
target = as.character(fit$terms[[2]])
rv = my.pred.stats(predict(fit,d,type="response"), d[,target], display = F)
return(rv$auc)
}
bs = boot(data=heart_test, statistic=boot.auc, R=5000, formula=HD ~. + SEX + CP + THALACH + OLDPEAK +
CA + THAL)
boot.ci(bs,conf=0.95,type="bca")
plot(bs)
bs = boot(data=heart_test[69,], statistic=boot.auc, R=5000, formula=HD ~. + SEX + CP + THALACH + OLDPEAK +
CA + THAL)
#1.1
housing <- read.csv("/Users/eugenefung/Desktop/Monash Things/School Work/Y2S2/FIT2086/Assignment 3/housing.ass3.2020.csv", header = T, stringsAsFactors = T)
housing
fit = lm(medv ~., housing)
summary(fit)
plot(predict(fit,housing), housing$medv)
pvalues = coefficients(summary(fit))[,4]
sum(pvalues<0.05)
#1.2
sum(pvalues<0.05/12)
#1.3
fit$coefficients
#1.4
fit_bic = step(fit, k = log(length(housing$medv)))
summary(fit_bic)
fit_bic$coefficients
#Qn 2.1
heart_train =  read.csv("/Users/eugenefung/Desktop/Monash Things/School Work/Y2S2/FIT2086/Assignment 3/heart.train.ass3.2020.csv")
heart_train
summary(heart_train)
#tree.heart_train = rpart(HD ~., heart_train)
#tree.heart_train
#plot(tree.heart_train)
#text(tree.heart_train, digits=3)
cv = learn.tree.cv(HD~.,data=heart_train,nfolds=10,m=5000)
plot.tree.cv(cv)
cv
#Qn 2.2
plot(cv$best.tree)
text(cv$best.tree,pretty=12)
#1.1
housing <- read.csv("/Users/eugenefung/Desktop/Monash Things/School Work/Y2S2/FIT2086/Assignment 3/housing.ass3.2020.csv", header = T, stringsAsFactors = T)
housing
fit = lm(medv ~., housing)
summary(fit)
plot(predict(fit,housing), housing$medv)
pvalues = coefficients(summary(fit))[,4]
sum(pvalues<0.05)
#1.2
sum(pvalues<0.05/12)
#1.2
sum(pvalues<0.05/12)
#1.3
fit$coefficients
#1.4
fit_bic = step(fit, k = log(length(housing$medv)))
summary(fit_bic)
fit_bic$coefficients
#1.6
new_sub = data.frame(0, 0.573, 6.03, 2.505, 21, 7.88)
names(new_sub) = c('chas', 'nox', 'rm', 'dis', 'ptratio', 'lstat')
new_sub
fit_bic
yhat = predict(fit_bic, new_sub)
yhat
#Qn 2.1
heart_train =  read.csv("/Users/eugenefung/Desktop/Monash Things/School Work/Y2S2/FIT2086/Assignment 3/heart.train.ass3.2020.csv")
heart_train
summary(heart_train)
#tree.heart_train = rpart(HD ~., heart_train)
#tree.heart_train
#plot(tree.heart_train)
#text(tree.heart_train, digits=3)
cv = learn.tree.cv(HD~.,data=heart_train,nfolds=10,m=5000)
plot.tree.cv(cv)
cv
#Qn 2.2
plot(cv$best.tree)
text(cv$best.tree,pretty=12)
#Qn 2.5
fullmod = glm(HD ~., data=heart_train, family=binomial)
summary(fullmod)
step_fullmod_bic = step(fullmod, k = log(length(heart_train)), direction = "both", trace = 0)
summary(step_fullmod_bic)
#Qn 2.7
heart_test = read.csv("/Users/eugenefung/Desktop/Monash Things/School Work/Y2S2/FIT2086/Assignment 3/heart.test.ass3.2020.csv")
prob = predict(step_fullmod_bic, heart_test, type="response")
my.pred.stats(prob, heart_test$HD)
my.pred.stats(predict(cv$best.tree, heart_test)[,2], heart_test$HD)
#Qn 2.8
heart_test
table(predict(cv$best.tree, heart_test, type = "class"), heart_test$HD)
pred = factor(prob>1/2,c(F,T),c("N","Y"))
table(pred, heart_test$HD)
pred = factor(prob>1/2,c(F,T),c("p","Y"))
table(pred, heart_test$HD)
table(pred, heart_test$HD)
jjj
table(pred, heart_test$HD)
#Qn 2.8
heart_test
table(predict(cv$best.tree, heart_test, type="class"), heart_test$HD)
pred = factor(prob>1/2,c(F,T),c("N","Y"))
table(pred, heart_test$HD)
#Qn 3.1.a
ms_train = read.csv("/Users/eugenefung/Desktop/Monash Things/School Work/Y2S2/FIT2086/Assignment 3/ms.train.ass3.2020.csv")
ms_train
knn = train.kknn(ms_train$MZ ~ ., data = ms_train, kmax=25, kernel= "optimal")
ytest.hat = fitted( kknn(ms_test$MZ ~ ., ms_test, ms_train,
kernel = "optimal", k = knn$best.parameters$k) )
sqrt(mean((ytest.hat - ms_test$intensity)^2))
#Qn 3.1.a
ms_train = read.csv("/Users/eugenefung/Desktop/Monash Things/School Work/Y2S2/FIT2086/Assignment 3/ms.train.ass3.2020.csv")
ms_train
ms_test = read.csv("/Users/eugenefung/Desktop/Monash Things/School Work/Y2S2/FIT2086/Assignment 3/ms.test.ass3.2020.csv")
knn = train.kknn(ms_train$MZ ~ ., data = ms_train, kmax=25, kernel= "optimal")
ytest.hat = fitted( kknn(ms_test$MZ ~ ., ms_test, ms_train,
kernel = "optimal", k = knn$best.parameters$k) )
sqrt(mean((ytest.hat - ms_test$intensity)^2))
plot(knn)
#Qn 3.1.b
knn_1 = train.kknn(ms_train$MZ ~ ., data = ms_train, kmax=2, kernel= "optimal")
plot(ms_train$intensity, ms_test$intensity)
#Qn 3.1.b
knn1 = train.kknn(ms_train$MZ ~ ., data = ms_train, kmax=2, kernel= "optimal")
#Qn 3.1.b
knn1 = train.kknn(ms_train$MZ ~ ., data = ms_train, kmax=2, kernel= "optimal")
plot(knn1)
knn2 = train.kknn(ms_train$MZ ~ ., data = ms_train, kmax=5, kernel= "optimal")
plot(knn2)
knn3 = train.kknn(ms_train$MZ ~ ., data = ms_train, kmax=10, kernel= "optimal")
plot(knn3)
knn4 = train.kknn(ms_train$MZ ~ ., data = ms_train, kmax=25, kernel= "optimal")
#Qn 3.1.b
knn1 = train.kknn(ms_train$MZ ~ ., data = ms_train, kmax=2, kernel= "optimal")
plot(knn1)
knn2 = train.kknn(ms_train$MZ ~ ., data = ms_train, kmax=5, kernel= "optimal")
plot(knn2)
knn3 = train.kknn(ms_train$MZ ~ ., data = ms_train, kmax=10, kernel= "optimal")
plot(knn3)
knn4 = train.kknn(ms_train$MZ ~ ., data = ms_train, kmax=25, kernel= "optimal")
plot(knn4)
plot(knn1)
#Qn 3.1.b
#knn1 = train.kknn(ms_train$MZ ~ ., data = ms_train, kmax=2, kernel= "optimal")
#plot(knn1)
#knn2 = train.kknn(ms_train$MZ ~ ., data = ms_train, kmax=5, kernel= "optimal")
#plot(knn2)
#knn3 = train.kknn(ms_train$MZ ~ ., data = ms_train, kmax=10, kernel= "optimal")
#plot(knn3)
#knn4 = train.kknn(ms_train$MZ ~ ., data = ms_train, kmax=25, kernel= "optimal")
#plot(knn4)
ytest.hat1 = fitted( kknn(ms_test$MZ ~ ., ms_test, ms_train,kernel = "optimal", k = 2) )
plot(ytest.hat1)
plot(ytest.hat2)
ytest.hat2 = fitted( kknn(ms_test$MZ ~ ., ms_test, ms_train,kernel = "optimal", k = 5) )
plot(ytest.hat2)
plot(ytest.hat1)
plot(ytest.hat2)
plot(ytest.hat1)
plot(ytest.hat2)
plot(ytest.hat3)
ytest.hat3 = fitted( kknn(ms_test$MZ ~ ., ms_test, ms_train,kernel = "optimal", k = 10) )
ytest.hat4 = fitted( kknn(ms_test$MZ ~ ., ms_test, ms_train,kernel = "optimal", k = 25) )
plot(ytest.hat3)
plot(ytest.hat4)
plot(ytest.hat1)
plot(ytest.hat1)
plot(ytest.hat2)
plot(ytest.hat3)
plot(ytest.hat4)
setwd("~/Desktop/Monash Things/School Work/Y3S2/FIT3179 Data Visualisation/FIT3179-Data-Viz-Assignment2/data")
df = read.csv("aus-property-sales-sep2018-april2020.csv")
df = subset(df, c(df$suburb, df$state))
df = read.csv("aus-property-sales-sep2018-april2020.csv")
df2 = subset(df, select=c(df$suburb, df$state))
df = read.csv("aus-property-sales-sep2018-april2020.csv")
df2 = subset(df, select=c(df$suburb, df$state))
df2
df = read.csv("aus-property-sales-sep2018-april2020.csv")
df2 = subset(df, select=c(suburb, state))
df2
library(dplyr)
df3 = df2
df3 = df2%>%
filter(state=="VIC")
df4 = as.data.frame(df3$suburb)
df4
df4 = as.data.frame(table(df3$suburb))
df4
df4 = count(df3, 'suburb')
df4
df4 = table(df3$suburb)
df4
View(df3)
count(df3)
df = read.csv("aus-property-sales-sep2018-april2020.csv")
df2 = subset(df, select=c(suburb, state))
df2
library(dplyr)
df3 = df2%>%
filter(state=="VIC")
count(df3)
View(df3)
View(df3)
df4 = c(df3$suburb)
df4
df4 = as.data.frame(table(df3$suburb))
df4
df5 = = df4%>%
filter(Freq > 0)
df5 = = df4%>%
filter(df4$Freq > 0)
df5 = df4%>%
filter(df4$Freq > 0)
df5
View(df5)
write.csv(df5, "/Users/eugenefung/Desktop/Monash Things/School Work/Y3S2/FIT3179 Data Visualisation/vic_suburb_houses.csv")
df6 = as.data.frame(table(df$state))
df6 = as.data.frame(table(df$state))
df6
write.csv(df6, "/Users/eugenefung/Desktop/Monash Things/School Work/Y3S2/FIT3179 Data Visualisation/houses_sold_state.csv")
View(df5)
